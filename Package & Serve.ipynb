{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f0adf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df ready. shape: (3000, 20)\n",
      "Columns (first 10): ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup']\n"
     ]
    }
   ],
   "source": [
    "# Cell 0 — reload df and do the minimal cleaning we already decided earlier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) Load your CSV (use the same path you used before)\n",
    "df = pd.read_csv(r'C:\\Users\\allur\\OneDrive\\Desktop\\project\\customer_churn_sample_3000.csv')\n",
    "\n",
    "# 2) Convert TotalCharges to numeric and fill any NaNs from conversion\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "df['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].median())\n",
    "\n",
    "# 3) Drop non-predictive ID if present\n",
    "if 'customerID' in df.columns:\n",
    "    df.drop('customerID', axis=1, inplace=True)\n",
    "\n",
    "# 4) Trim whitespace in text columns\n",
    "for c in df.select_dtypes(include='object').columns:\n",
    "    df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# 5) Ensure target exists and is 0/1 (handle case-insensitive 'churn')\n",
    "churn_candidates = [c for c in df.columns if c.lower() == 'churn']\n",
    "assert churn_candidates, f\"Couldn't find a 'Churn' column. Available columns: {list(df.columns)}\"\n",
    "if churn_candidates[0] != 'Churn':\n",
    "    df.rename(columns={churn_candidates[0]:'Churn'}, inplace=True)\n",
    "\n",
    "if df['Churn'].dtype == 'object':\n",
    "    df['Churn'] = df['Churn'].map({'Yes':1, 'No':0}).astype(int)\n",
    "\n",
    "print(\"df ready. shape:\", df.shape)\n",
    "print(\"Columns (first 10):\", list(df.columns)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f180f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models present: {}\n",
      "'X_train' exists: False\n"
     ]
    }
   ],
   "source": [
    "# Cell A — memory check\n",
    "CANDIDATES = ['best_model','xgb','rf','lr','model']\n",
    "present = {n: type(globals()[n]).__name__ for n in CANDIDATES if n in globals()}\n",
    "print(\"Models present:\", present)\n",
    "print(\"'X_train' exists:\", 'X_train' in globals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c747097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline LogisticRegression trained; model + X_train ready ✅\n",
      "X_train shape: (2400, 30)\n"
     ]
    }
   ],
   "source": [
    "# Cell B — quick baseline to restore model + X_train\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# 1) sanity: df must exist\n",
    "assert 'df' in globals(), \"DataFrame df not found. Re-run your cleaning cell(s).\"\n",
    "\n",
    "# 2) ensure target is numeric 0/1\n",
    "assert 'Churn' in df.columns, \"Expected a 'Churn' column.\"\n",
    "if df['Churn'].dtype == 'object':\n",
    "    df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0}).astype(int)\n",
    "\n",
    "# 3) one-hot encode remaining object columns (exclude target)\n",
    "obj_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "if 'Churn' in obj_cols:\n",
    "    obj_cols.remove('Churn')\n",
    "df_enc = pd.get_dummies(df, columns=obj_cols, drop_first=True)\n",
    "\n",
    "# 4) split\n",
    "X = df_enc.drop(columns=['Churn'])\n",
    "y = df_enc['Churn']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 5) optional scaling for common numeric columns (if present)\n",
    "num_cols = [c for c in ['tenure','MonthlyCharges','TotalCharges'] if c in X_train.columns]\n",
    "scaler = None\n",
    "if num_cols:\n",
    "    scaler = StandardScaler()\n",
    "    X_train.loc[:, num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "    X_test.loc[:,  num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "# 6) train a quick baseline model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# expose variables your save-cell expects\n",
    "model = lr  # your save-cell will find this\n",
    "print(\"Baseline LogisticRegression trained; model + X_train ready ✅\")\n",
    "print(\"X_train shape:\", X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0adbf34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using trained model variable: lr\n",
      "Saved:\n",
      " - models/model_20250831_002611.pkl\n",
      " - models/scaler_20250831_002611.pkl\n",
      " - models/columns_20250831_002611.json\n"
     ]
    }
   ],
   "source": [
    "# === Sprint 5 · Cell 1: Save artifacts (model, scaler, feature schema) ===\n",
    "from pathlib import Path\n",
    "import joblib, json, time\n",
    "\n",
    "# 1) pick your trained model variable automatically\n",
    "CANDIDATES = ['best_model', 'xgb', 'rf', 'lr', 'model']\n",
    "model = None\n",
    "for name in CANDIDATES:\n",
    "    if name in globals():\n",
    "        model = globals()[name]\n",
    "        print(f\"Using trained model variable: {name}\")\n",
    "        break\n",
    "assert model is not None, f\"No trained model variable found. Expected one of: {CANDIDATES}\"\n",
    "\n",
    "# 2) get feature columns in the exact order used for training\n",
    "assert 'X_train' in globals(), \"X_train not found. Keep X_train in memory (with the final encoded columns) before saving artifacts.\"\n",
    "feature_columns = list(X_train.columns)\n",
    "\n",
    "# 3) optional scaler if you used one\n",
    "scaler_obj = globals().get('scaler', None)\n",
    "\n",
    "# 4) make models/ and build stamped filenames\n",
    "Path(\"models\").mkdir(exist_ok=True)\n",
    "stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path  = f\"models/model_{stamp}.pkl\"\n",
    "scaler_path = f\"models/scaler_{stamp}.pkl\" if scaler_obj is not None else None\n",
    "schema_path = f\"models/columns_{stamp}.json\"\n",
    "\n",
    "# 5) save artifacts\n",
    "joblib.dump(model, model_path)\n",
    "if scaler_obj is not None:\n",
    "    joblib.dump(scaler_obj, scaler_path)\n",
    "\n",
    "schema = {\n",
    "    \"feature_columns\": feature_columns,\n",
    "    \"model_path\": model_path,\n",
    "    \"scaler_path\": scaler_path,\n",
    "    \"created_at\": stamp\n",
    "}\n",
    "with open(schema_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(schema, f, indent=2)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", model_path)\n",
    "if scaler_path: print(\" -\", scaler_path)\n",
    "print(\" -\", schema_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
